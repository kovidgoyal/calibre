# vim:fileencoding=UTF-8:ts=4:sw=4:sta:et:sts=4:ai
from __future__ import unicode_literals
from calibre.web.feeds.recipes import BasicNewsRecipe

class ceskaPoziceRecipe(BasicNewsRecipe):
    __author__  = 'bubak'
    title = u'Česká pozice'
    description = 'Česká pozice'
    oldest_article = 2
    max_articles_per_feed = 20

    feeds = [
        (u'Všechny články', u'http://www.ceskapozice.cz/rss.xml'),
        (u'Domov', u'http://www.ceskapozice.cz/taxonomy/term/16/feed'),
        (u'Chrono', u'http://www.ceskapozice.cz/chrono/feed'),
        (u'Evropa', u'http://www.ceskapozice.cz/taxonomy/term/17/feed')
            ]


    language = 'cs'
    cover_url = 'http://www.ceskapozice.cz/sites/default/files/cpozice_logo.png'
    remove_javascript = True
    no_stylesheets = True
    domain = u'http://www.ceskapozice.cz'
    use_embedded_content = False


    remove_tags = [dict(name='div',   attrs={'class':['block-ad', 'region region-content-ad']}),
               dict(name='ul',   attrs={'class':'links'}),
               dict(name='div',   attrs={'id':['comments', 'back-to-top']}),
               dict(name='div',   attrs={'class':['next-page', 'region region-content-ad']}),
           dict(name='cite')]

    keep_only_tags = [dict(name='div',   attrs={'id':'content'})]

    visited_urls = {}
    def get_article_url(self, article):
        url = BasicNewsRecipe.get_article_url(self, article)
        if url in self.visited_urls:
            self.log.debug('Ignoring duplicate: ' + url)
            return None
        else:
            self.visited_urls[url] = True
            self.log.debug('Accepting: ' + url)
            return url

    def preprocess_html(self, soup):
        self.append_page(soup, soup.body, 3)
        return soup

    def append_page(self, soup, appendtag, position):
        pager = soup.find('div', attrs={'class':'paging-bottom'})
        if pager:
            nextbutton = pager.find('li', attrs={'class':'pager-next'})
            if nextbutton:
                nexturl = self.domain + nextbutton.a['href']
                soup2 = self.index_to_soup(nexturl)
                texttag = soup2.find('div', attrs={'class':'main-body'})
                for it in texttag.findAll('div', attrs={'class':'region region-content-ad'}):
                    it.extract()
                    for it in texttag.findAll('cite'):
                        it.extract()
                        newpos = len(texttag.contents)
                        self.append_page(soup2, texttag, newpos)
                        texttag.extract()
                        appendtag.insert(position, texttag)
                        pager.extract()

