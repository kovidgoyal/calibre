#!/usr/bin/env python2
# vim:fileencoding=UTF-8:ts=4:sw=4:sta:et:sts=4:fdm=marker:ai
from __future__ import (unicode_literals, division, absolute_import,
                        print_function)
from collections import defaultdict
from pprint import pformat
from calibre.web.feeds.news import BasicNewsRecipe

def absurl(url):
    if url.startswith('/'):
        url = 'http://www.latimes.com' + url
    return url

class LATimes(BasicNewsRecipe):
    title                 = 'Los Angeles Times'
    __author__            = 'Kovid Goyal'
    description           = 'The Los Angeles Times is a leading source of news on Southern California, entertainment, movies, television, music, politics, business, health, technology, travel, sports, environment, economics, autos, jobs, real estate and other topics affecting California'  # noqa
    category              = 'news, politics, USA, Los Angeles, world'
    oldest_article        = 2
    max_articles_per_feed = 200
    no_stylesheets        = True
    encoding              = 'utf8'
    use_embedded_content  = False
    language              = 'en'
    remove_empty_feeds    = True
    publication_type      = 'newspaper'
    masthead_url          = 'http://www.latimes.com/images/logo.png'
    cover_url             = 'http://www.latimes.com/includes/sectionfronts/A1.pdf'

    keep_only_tags = [
        dict(itemprop='articleBody'),
        dict(name='h1'),
        dict(attrs={'data-content-type':'image'}),
    ]
    remove_tags = [
        dict(attrs={'data-content-type':'story'}),
        dict(attrs={'data-load-type':'commentFrame'}),
    ]

    def parse_index(self):
        soup = self.index_to_soup('http://www.latimes.com')
        feeds = defaultdict(list)
        for x in soup.findAll(attrs={'data-content-type':'story', 'data-content-section':True, 'data-content-url':True, 'data-content-title':True}):
            url = absurl(x['data-content-url'])
            section = x['data-content-section'].capitalize()
            title = x['data-content-title']
            feeds[section].append({'title':title, 'url':url})
        self.log(pformat(dict(feeds)))
        return [(k, feeds[k]) for k in sorted(feeds)]

    def preprocess_html(self, soup):
        for img in soup.findAll('img', attrs={'data-baseurl':True}):
            img['src'] = img['data-baseurl'] + '/750'
        for div in soup.findAll(itemprop='articleBody'):
            div.extract()
            soup.find('body').append(div)
        return soup
