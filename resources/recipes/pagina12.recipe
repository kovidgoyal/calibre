__license__   = 'GPL v3'
__copyright__ = '2008-2010, Darko Miletic <darko.miletic at gmail.com>'
'''
pagina12.com.ar
'''

import re
from calibre.web.feeds.news import BasicNewsRecipe
from calibre.ebooks.BeautifulSoup import BeautifulSoup

class Pagina12(BasicNewsRecipe):
    title                 = 'Pagina - 12'
    __author__            = 'Darko Miletic'
    description           = 'Noticias de Argentina y el resto del mundo'
    publisher             = 'La Pagina S.A.'
    category              = 'news, politics, Argentina'
    oldest_article        = 2
    max_articles_per_feed = 200
    no_stylesheets        = True
    encoding              = 'cp1252'
    use_embedded_content  = False
    language              = 'es'
    remove_empty_feeds    = True
    publication_type      = 'newspaper'
    masthead_url          = 'http://www.pagina12.com.ar/commons/imgs/logo-home.gif'
    extra_css             = """ 
                               body{font-family: Arial,Helvetica,sans-serif } 
                               img{margin-bottom: 0.4em; display:block}
                               #autor{font-weight: bold} 
                               #fecha,#epigrafe{font-size: 0.9em; margin: 5px} 
                               #imagen{border: 1px solid black; margin: 0 0 1.25em 1.25em; width: 232px }
                               .fgprincipal{font-size: large; font-weight: bold}
                            """

    conversion_options = {
                          'comment'   : description
                        , 'tags'      : category
                        , 'publisher' : publisher
                        , 'language'  : language
                        }

    remove_tags = [
                     dict(name=['meta','link'])
                    ,dict(name='div', attrs={'id':['volver','logo','logo_suple','fin','permalink']})
                  ]
    remove_attributes=['lang']


    feeds = [
              (u'Edicion impresa', u'http://www.pagina12.com.ar/diario/rss/principal.xml'   )
             ,(u'Espectaculos'   , u'http://www.pagina12.com.ar/diario/rss/espectaculos.xml')
             ,(u'Radar'          , u'http://www.pagina12.com.ar/diario/rss/radar.xml'       )
             ,(u'Radar libros'   , u'http://www.pagina12.com.ar/diario/rss/libros.xml'      )
             ,(u'Cash'           , u'http://www.pagina12.com.ar/diario/rss/cash.xml'        )
             ,(u'Turismo'        , u'http://www.pagina12.com.ar/diario/rss/turismo.xml'     )
             ,(u'Libero'         , u'http://www.pagina12.com.ar/diario/rss/libero.xml'      )
             ,(u'NO'             , u'http://www.pagina12.com.ar/diario/rss/no.xml'          )
             ,(u'Las/12'         , u'http://www.pagina12.com.ar/diario/rss/las12.xml'       )
             ,(u'Soy'            , u'http://www.pagina12.com.ar/diario/rss/soy.xml'         )
             ,(u'Futuro'         , u'http://www.pagina12.com.ar/diario/rss/futuro.xml'      )
             ,(u'M2'             , u'http://www.pagina12.com.ar/diario/rss/m2.xml'          )
             ,(u'Rosario/12'     , u'http://www.pagina12.com.ar/diario/rss/rosario.xml'     )
            ]

    def print_version(self, url):
        return url.replace('http://www.pagina12.com.ar/','http://www.pagina12.com.ar/imprimir/')

    def get_cover_url(self):
        rawc = self.index_to_soup('http://www.pagina12.com.ar/diario/principal/diario/index.html',True)
        rawc2 = re.sub(r'PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN','PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"',rawc)
        soup = BeautifulSoup(rawc2,fromEncoding=self.encoding,smartQuotesTo=None)
        for image in soup.findAll('img',alt=True):
           if image['alt'].startswith('Tapa de la fecha'):
              return image['src']
        return None

    def preprocess_html(self, soup):
        for item in soup.findAll(style=True):
            del item['style']
        for item in soup.findAll('span', attrs={'id':'seccion'}):
            it = item.a
            it.name='span'
            del it['href']
            del it['title']
        for item in soup.findAll('p'):
            it = item.find('h3')            
            if it:
               it.name='span'
        return soup